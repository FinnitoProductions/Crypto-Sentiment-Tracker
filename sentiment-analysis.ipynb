{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the NLP server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<os._wrap_close at 0x1131f8d68>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "STANFORD_NLP_LOCATION = '~/stanford-corenlp-full-2018-10-05/' # the location of the Stanford NLP library on my computer \n",
    "                                                              #(download at https://stanfordnlp.github.io/CoreNLP)\n",
    "STANFORD_NLP_TIMEOUT = 10000 # the time after which a given NLP request will be killed if not yet complete\n",
    "\n",
    "exec = os.popen\n",
    "exec('cd %s; java -mx5g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout %s' % \n",
    "     (STANFORD_NLP_LOCATION, STANFORD_NLP_TIMEOUT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create function to determine the sentiment of a block of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "import re\n",
    "import numpy\n",
    "\n",
    "# Determines the sentiment of a given block of text as a range from 0 (Extremely Negative) to 4 (Extremely Positive).\n",
    "def findSentiment(text):\n",
    "    NLP_SERVER_LOCATION = 'http://localhost:9000'\n",
    "    PROPERTIES_DICTIONARY = {'annotators': 'sentiment', 'outputFormat': 'json', 'timeout': 1000}\n",
    "    \n",
    "    nlp = StanfordCoreNLP(NLP_SERVER_LOCATION)\n",
    "    result = nlp.annotate(text, properties = PROPERTIES_DICTIONARY)\n",
    "\n",
    "    sentiments = []\n",
    "    for sentenceAnalysis in result['sentences']:\n",
    "        sentence = \" \".join(t['word'] for t in sentenceAnalysis['tokens'])\n",
    "        sentence = sentence.replace('.', '')\n",
    "\n",
    "        sentimentValue = float(sentenceAnalysis['sentimentValue'])\n",
    "\n",
    "        sentiments += [sentimentValue]\n",
    "    return numpy.average(sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape web for news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "def getPageContent(url):\n",
    "    try: \n",
    "        with closing(get(url, stream = True)) as page:\n",
    "            return page.content.decode(\"utf-8\")\n",
    "    except RequestException as e:\n",
    "        print(e)\n",
    "        return\n",
    "    \n",
    "def parseHTML(url):\n",
    "    return BeautifulSoup(getPageContent(url), 'html.parser')\n",
    "\n",
    "# Extracts the text from a CNN article with given URL, excluding the headline and any advertisements.\n",
    "def getCNNText(url):\n",
    "    htmlParser = parseHTML(url)\n",
    "    \n",
    "    text = \"\"\n",
    "\n",
    "    for element in htmlParser.select('div'):\n",
    "        if element.has_attr('class') and 'zn-body__paragraph' in element['class']: \n",
    "            text += element.text \n",
    "    return text\n",
    "\n",
    "# Uses NewsAPI to extract all articles from a given day.\n",
    "def getArticleURLs(day, month, year):\n",
    "    dayPrefix = \"%04d-%02d-%02d\" % (year, month, day)\n",
    "    \n",
    "    newsapi = NewsApiClient(api_key = '8d99d69a251a453f8c084f4768db7195')\n",
    "    \n",
    "    urls = []\n",
    "    second = 0\n",
    "    minute = 0\n",
    "    for hour in range (0, 24):\n",
    "        for minute in range (0, 59, 10):\n",
    "            iso8601DateStart = \"%sT%s\" % (dayPrefix, \"%02d:%02d:%02d\" % (hour, minute, second))\n",
    "            iso8601DateEnd = \"%sT%s\" % (dayPrefix, \"%02d:%02d:%02d\" % (hour, minute + (10 if minute != 50 else 9), second))\n",
    "            all_articles = newsapi.get_everything(q='bitcoin', from_param= iso8601DateStart, to=iso8601DateEnd, page=1)\n",
    "            for article in all_articles['articles']:\n",
    "                urls += article['url']\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kill the NLP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<os._wrap_close at 0x1131f8198>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec('kill $(lsof -ti tcp:9000)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
